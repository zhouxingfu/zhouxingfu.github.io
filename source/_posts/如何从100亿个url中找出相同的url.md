---
title: 如何从100亿个url中找出相同的url
date: 2020-09-21 12:17:33
tags:
categories: [algorithm]
---
看到这道题目的第一反应，HashMap，但100亿真的太多了，我不可能在内存中建立一个这个大的HashMap。

这里面需要的操作是
1. 计算所有文件的hash（还要解决可能出现的哈希冲突）
2. 内存中要保存一个几百G的哈希表

查找的时候因为是哈希，所以不需要遍历匹配，所以这块消耗不大。

但内存就否决了方案。  

一个优化思路是 计算出hash之后%1000，这样可以分成1000个文件夹，这次每次处理其中一个文件夹就好。  


但这里也要问题
1. 如何保证尽量均为分布到1000个文件夹？很难，尽量选用合适的哈希算法。  
2. IO操作大，因为计算哈希之后要先保存下来，再%1000。  

还有一个思路，就是外部排序，然后逐个比较，但外部排序100亿个，O(NlogN)，而且也不能一次加载到内存中。 

从知乎一个回答上看可以用布隆filter，而这又是redis里的概念，所以
```
mapreduce redis是怎么实现的呢？
```
<!--more-->
